{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video autoplay\n",
       " width=400 height=300 style='cursor: pointer;'></video>\n",
       "<script>\n",
       "\n",
       "var video = document.querySelector('video')\n",
       "\n",
       "navigator.mediaDevices.getUserMedia({ video: true })\n",
       "  .then(stream=> video.srcObject = stream)\n",
       "\n",
       "var data = new Promise(resolve=>{\n",
       "  video.onclick = ()=>{\n",
       "    var canvas = document.createElement('canvas')\n",
       "    var [w,h] = [video.offsetWidth, video.offsetHeight]\n",
       "    canvas.width = w\n",
       "    canvas.height = h\n",
       "    canvas.getContext('2d')\n",
       "          .drawImage(video, 0, 0, w, h)\n",
       "    video.srcObject.getVideoTracks()[0].stop()\n",
       "    video.replaceWith(canvas)\n",
       "    resolve(canvas.toDataURL('image/jpeg', 2.000000))\n",
       "  }\n",
       "})\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'eval_js' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 72\u001b[0m\n\u001b[0;32m     67\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(Image\u001b[38;5;241m.\u001b[39mopen(f))\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m#Capturando a foto\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Clique na imagem da webcam para tirar uma foto\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m imagem \u001b[38;5;241m=\u001b[39m \u001b[43mtirar_foto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Inverte a ordem dos canais (utilizar caso a imagem capturada fique com cores invertidas)\u001b[39;00m\n\u001b[0;32m     74\u001b[0m imagem \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(imagem, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n",
      "Cell \u001b[1;32mIn[20], line 64\u001b[0m, in \u001b[0;36mtirar_foto\u001b[1;34m(filename, quality, size)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtirar_foto\u001b[39m(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphoto.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, quality\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m400\u001b[39m,\u001b[38;5;241m300\u001b[39m)):\n\u001b[0;32m     63\u001b[0m   display(HTML(VIDEO_HTML \u001b[38;5;241m%\u001b[39m (size[\u001b[38;5;241m0\u001b[39m],size[\u001b[38;5;241m1\u001b[39m],quality)))\n\u001b[1;32m---> 64\u001b[0m   data \u001b[38;5;241m=\u001b[39m \u001b[43meval_js\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m   binary \u001b[38;5;241m=\u001b[39m b64decode(data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     66\u001b[0m   f \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(binary)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'eval_js' is not defined"
     ]
    }
   ],
   "source": [
    "#Atividade de experimentação 67\n",
    "#Testando o modelo do detector de emoções pela webcam\n",
    "\n",
    "#Importando as bibliotecas\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from google.colab.patches import cv2_imshow\n",
    "import zipfile\n",
    "#%tensorflow_version 2.x\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "tensorflow.__version__\n",
    "\n",
    "#Conectando com o Drive e acessando os arquivos\n",
    "# Conectando o Colab ao Google Drive\n",
    "# Realize o dowload da pasta Material_complementar_reconhecimento_emocoes.zip do Google Sala de Aula e transfira-a para o seu Google Drive\n",
    "# Localize o caminho da pasta no menu Arquivos, no menu lateral esquerdo\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "#path = \"/content/gdrive/MyDrive/Material_complementar_reconhecimento_emocoes.zip\"\n",
    "#zip_object = zipfile.ZipFile(file=path, mode=\"r\")\n",
    "#zip_object.extractall(\"./\")\n",
    "#zip_object.close\n",
    "\n",
    "#Testando a foto capturada da webcam\n",
    "# Script para visualização de vídeo pela webcam\n",
    "from IPython.display import HTML, Audio\n",
    "#from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "import numpy as np\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "VIDEO_HTML = \"\"\"\n",
    "<video autoplay\n",
    " width=%d height=%d style='cursor: pointer;'></video>\n",
    "<script>\n",
    "\n",
    "var video = document.querySelector('video')\n",
    "\n",
    "navigator.mediaDevices.getUserMedia({ video: true })\n",
    "  .then(stream=> video.srcObject = stream)\n",
    "\n",
    "var data = new Promise(resolve=>{\n",
    "  video.onclick = ()=>{\n",
    "    var canvas = document.createElement('canvas')\n",
    "    var [w,h] = [video.offsetWidth, video.offsetHeight]\n",
    "    canvas.width = w\n",
    "    canvas.height = h\n",
    "    canvas.getContext('2d')\n",
    "          .drawImage(video, 0, 0, w, h)\n",
    "    video.srcObject.getVideoTracks()[0].stop()\n",
    "    video.replaceWith(canvas)\n",
    "    resolve(canvas.toDataURL('image/jpeg', %f))\n",
    "  }\n",
    "})\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "def tirar_foto(filename='photo.jpg', quality=2, size=(400,300)):\n",
    "  display(HTML(VIDEO_HTML % (size[0],size[1],quality)))\n",
    "  data = eval_js(\"data\")\n",
    "  binary = b64decode(data.split(',')[1])\n",
    "  f = io.BytesIO(binary)\n",
    "  return np.asarray(Image.open(f))\n",
    "\n",
    "\n",
    "#Capturando a foto\n",
    "# Clique na imagem da webcam para tirar uma foto\n",
    "imagem = tirar_foto()\n",
    "# Inverte a ordem dos canais (utilizar caso a imagem capturada fique com cores invertidas)\n",
    "imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
    "#cv2_imshow(imagem)\n",
    "plt.imshow(imagem)\n",
    "plt.axis('off')  # Desativar eixos para uma visualização mais limpa\n",
    "plt.show()\n",
    "cv2.imwrite(\"testecaptura.jpg\",imagem)\n",
    "\n",
    "# Utilize um haarcasdade pré treinado para o reconhecimento facial\n",
    "# Utilize um modelo pré treinado para o reconhecimento das emoções\n",
    "#Carreganos os diretorios de imagens e modelos \n",
    "# Caminhos dos arquivos\n",
    "dir_cascade_faces =  'D:/Dados/Material_complementar_reconhecimento_emocoes/haarcascade_frontalface_default.xml'\n",
    "dir_modelo_emotion = 'D:/Dados/Material_complementar_reconhecimento_emocoes/modelo_01_expressoes.h5'\n",
    "\n",
    "# Outras partes do seu código aqui (por exemplo, carregar o classificador de faces, etc.)\n",
    "cascade_faces = dir_cascade_faces\n",
    "caminho_modelo = dir_modelo_emotion\n",
    "\n",
    "cascade_faces = dir_cascade_faces\n",
    "caminho_modelo = dir_modelo_emotion\n",
    "face_detection = cv2.CascadeClassifier(cascade_faces)\n",
    "classificador_emocoes = load_model(caminho_modelo, compile=False)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Carrega o modelo\n",
    "face_detection = cv2.CascadeClassifier(cascade_faces)\n",
    "classificador_emocoes = load_model(caminho_modelo, compile=False)\n",
    "# Classifica cada uma das categorias das expressões\n",
    "expressoes = [\"Raiva\", \"Nojo\", \"Medo\", \"Feliz\", \"Triste\", \"Surpreso\", \"Neutro\"]\n",
    "\n",
    "original = imagem.copy()\n",
    "faces = face_detection.detectMultiScale(original,scaleFactor=1.1,minNeighbors=3,minSize=(20,20))\n",
    "cinza = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "if len(faces) > 0:\n",
    "    for (fX, fY, fW, fH) in faces:\n",
    "      roi = cinza[fY:fY + fH, fX:fX + fW]\n",
    "      roi = cv2.resize(roi, (48, 48))\n",
    "      roi = roi.astype(\"float\") / 255.0\n",
    "      roi = img_to_array(roi)\n",
    "      roi = np.expand_dims(roi, axis=0)\n",
    "      preds = classificador_emocoes.predict(roi)[0]\n",
    "      print(preds)\n",
    "      emotion_probability = np.max(preds)\n",
    "      label = expressoes[preds.argmax()]\n",
    "      cv2.putText(original, label, (fX, fY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "      cv2.rectangle(original, (fX, fY), (fX + fW, fY + fH),(0, 0, 255), 2)\n",
    "else:\n",
    "    print('Nenhuma face detectada')\n",
    "\n",
    "\n",
    "#cv2_imshow(original)\n",
    "plt.imshow(original)\n",
    "plt.axis('off')  # Desativar eixos para uma visualização mais limpa\n",
    "plt.show()\n",
    "\n",
    "probabilidades = np.ones((250, 300, 3), dtype=\"uint8\") * 255\n",
    "# Mostra gráfico apenas se detectou uma face\n",
    "if len(faces) == 2:\n",
    "  for (i, (emotion, prob)) in enumerate(zip(expressoes, preds)):\n",
    "      # Nome das emoções\n",
    "      text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
    "      w = int(prob * 300)\n",
    "      cv2.rectangle(probabilidades, (7, (i * 35) + 5),\n",
    "      (w, (i * 35) + 35), (200, 250, 20), -1)\n",
    "      cv2.putText(probabilidades, text, (10, (i * 35) + 23),\n",
    "      cv2.FONT_HERSHEY_SIMPLEX, 0.45,\n",
    "      (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "  #cv2_imshow(probabilidades)\n",
    "  plt.imshow(probabilidades)\n",
    "  plt.axis('off')  # Desativar eixos para uma visualização mais limpa\n",
    "  plt.show()\n",
    "\n",
    "cv2.imwrite(\"captura.jpg\",original)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
