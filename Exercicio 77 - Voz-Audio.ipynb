{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atividade de experimentação 77 Reconhecimento de emoções de fala RAVDESS - SER: Speech Emotion Recognition\n",
    "\n",
    "Base de dados: Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)\n",
    "\n",
    "Contém 7356 arquivos (tamanho total: 24,8 GB); O banco de dados contém 24 atores profissionais (12 mulheres, 12 homens), vocalizando duas declarações lexicalmente combinadas em um sotaque norte-americano neutro; falando as expressões em entonações calmas, felizes, tristes, zangadas, com medo, surpresa e desgosto ou cantando as expressões em entonações calmas, felizes, tristes, raivosas e temerosas assim totalizando 8 emoções; image.png\n",
    "\n",
    "Cada expressão é produzida em dois níveis de intensidade emocional (normal, forte), com uma expressão neutra adicional.\n",
    "\n",
    "Toda a base de dados está subdividida em 3 modalidades:\n",
    "\n",
    "somente áudio (16 bits, 48kHz .wav) áudio-vídeo (720p H.264, AAC 48kHz, .mp4) somente vídeo (sem som). Nota: Não há arquivos de música para Actor_18.\n",
    "\n",
    "Mais detalhes sobre a base de dados podem ser vistos na página oficial RAVDESS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] tensorflow: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "#Importando as bibliotecas\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "import librosa\n",
    "import librosa.display as ld\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Audio\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "print(f'[INFO] tensorflow: {tf.__version__}')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import (Activation, Conv1D, Dense, Dropout, Flatten, MaxPooling1D)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análise exploratória de dados (EDA)\n",
    "Baixando e carregando os datasets: SPEECH/SONG\n",
    "\n",
    "Neste projeto serão usados somente os arquivos de áudio contendo as expressões faladas(speech) e cantadas(song) devido a limitação de espaço disponível no Google Drive.\n",
    "\n",
    "arquivos de fala (Audio_Speech_Actors) contém 1440 arquivos;\n",
    "arquivos de música (Audio_Song_Actors) contém 1012 arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código para baixar e extrair o dataset de áudio\n",
    "#!wget https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip -O Audio_Speech_Actors_01-24.zip\n",
    "#!unzip Audio_Speech_Actors_01-24.zip -d '/content/Audio_Speech_Actors'\n",
    "#!rm Audio_Speech_Actors_01-24.zip\n",
    "## Comando para Colab \n",
    "\n",
    "##Comando para Vscode\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# URL do arquivo zip\n",
    "url = \"https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip\"\n",
    "\n",
    "# Nome do arquivo zip\n",
    "zip_filename = \"Audio_Speech_Actors_01-24.zip\"\n",
    "\n",
    "# Diretório de destino para extrair o arquivo zip\n",
    "extract_dir = \"D:/Dados/Material_complementar_reconhecimento_voz/Audio_Speech_Actors\"\n",
    "\n",
    "# Baixar o arquivo zip\n",
    "urllib.request.urlretrieve(url, zip_filename)\n",
    "\n",
    "# Extrair o arquivo zip\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "# Remover o arquivo zip após a extração\n",
    "os.remove(zip_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificadores de nome de arquivo Cada um dos 7356 arquivos RAVDESS possui um nome de arquivo exclusivo. O nome do arquivo consiste em um identificador numérico de 7 partes (por exemplo, 03-02-01-01-01-01-01.wav). Esses identificadores definem as características do estímulo.\n",
    "\n",
    "Modalidade (01 = AV completo, 02 = apenas vídeo, 03 = apenas áudio). Canal vocal (01 = fala, 02 = música). Emoção (01 = neutro, 02 = calmo, 03 = feliz, 04 = triste, 05 = zangado, 06 = com medo, 07 = nojo, 08 = surpreso). Intensidade emocional (01 = normal, 02 = forte). NOTA: Não há intensidade forte para a emoção 'neutra'. Frase (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\"). Repetição (01 = 1ª repetição, 02 = 2ª repetição). Ator (01 a 24. Os atores com números ímpares são homens, os atores com números pares são mulheres). Exemplo para o arquivo Audio_Song_Actors/Actor_01/03-02-01-01-01-01-01.wav:\n",
    "\n",
    "Modalidade 03: Apenas áudio Canal vocal 02: música Emoção 01: neutro Intensidade emocional 01: normal. NOTA: Não há intensidade forte para a emoção 'neutra'. Frase 01: \"Kids are talking by the door\" Repetição 01: 1ª repetição Ator 01: 1º ator - homem, já que o número de identificação do ator é impar'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando os datasets\n",
    "modalitys = [] # Modalidade (01 = AV completo, 02 = apenas vídeo, 03 = apenas áudio).\n",
    "voc_channels = [] # Canal vocal (01 = fala, 02 = música).\n",
    "emotions = [] # Emoção (01 = neutro, 02 = calma, 03 = feliz, 04 = triste, 05 = zangado, 06 = com medo, 07 = nojo, 08 = surpreso).\n",
    "intensitys = [] # Intensidade emocional (01 = normal, 02 = forte). NOTA: Não há intensidade forte para a emoção 'neutra'.\n",
    "phrases =[] # Frase (01 = \"Crianças conversam perto da porta\", 02 = \"Cachorros estão sentados na porta\").\n",
    "actors = [] # Ator (01 a 24. Os atores com números ímpares são homens, os atores com números pares são mulheres)\n",
    "\n",
    "full_path = []\n",
    "\n",
    "# Função para criar o dataset\n",
    "def create_dataset(dataset):\n",
    "  for root, dirs, files in tqdm(os.walk(dataset)):\n",
    "    for file in files:\n",
    "      try:\n",
    "        modal = int(file[1:2])\n",
    "        vchan = int(file[4:5])\n",
    "        label = int(file[7:8])\n",
    "        ints = int(file[10:11])\n",
    "        phr = int(file[13:14])\n",
    "        act = int(file[19:20])\n",
    "\n",
    "        modalitys.append(modal)\n",
    "        voc_channels.append(vchan)\n",
    "        emotions.append(label)\n",
    "        intensitys.append(ints)\n",
    "        phrases.append(phr)\n",
    "        actors.append(act)\n",
    "\n",
    "        full_path.append((root, file))\n",
    "      except ValueError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Cria datasets a partir dos arquivos de áudio\n",
    "create_dataset(\"D:/Dados/Material_complementar_reconhecimento_voz/Audio_Speech_Actors/Audio_Speech_Actors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostra o tamanho do dataset de áudio\n",
    "len(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'neutra',\n",
       " 2: 'calma',\n",
       " 3: 'feliz',\n",
       " 4: 'triste',\n",
       " 5: 'nervosa',\n",
       " 6: 'medo',\n",
       " 7: 'nojo',\n",
       " 8: 'surpreso'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapeia emoções para rótulos textuais\n",
    "emotions_list = ['neutra', 'calma', 'feliz', 'triste', 'nervosa', 'medo', 'nojo', 'surpreso']\n",
    "emotion_dict = {em[0] + 1:em[1] for em in enumerate(emotions_list)}\n",
    "emotion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>voc_channel</th>\n",
       "      <th>modality</th>\n",
       "      <th>intensity</th>\n",
       "      <th>actors</th>\n",
       "      <th>phrase</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [emotion, voc_channel, modality, intensity, actors, phrase, path]\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cria DataFrame com as informações extraídas\n",
    "df = pd.DataFrame([emotions, voc_channels, modalitys, intensitys, actors, phrases, full_path]).T\n",
    "df.columns = ['emotion', 'voc_channel', 'modality', 'intensity', 'actors', 'phrase', 'path']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>voc_channel</th>\n",
       "      <th>modality</th>\n",
       "      <th>intensity</th>\n",
       "      <th>actors</th>\n",
       "      <th>phrase</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [emotion, voc_channel, modality, intensity, actors, phrase, path]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapeia valores numéricos para rótulos de texto no DataFrame\n",
    "df['emotion'] = df['emotion'].map(emotion_dict)\n",
    "df['voc_channel'] = df['voc_channel'].map({1: 'fala', 2: 'musica'})\n",
    "df['modality'] = df['modality'].map({1: 'AV completo', 2: 'apenas video', 3: 'apenas audio'})\n",
    "df['intensity'] = df['intensity'].map({1: 'normal', 2: 'forte'})\n",
    "df['actors'] = df['actors'].apply({lambda x: 'feminino' if x % 2 == 0 else 'masculino'})\n",
    "df['phrase'] = df['phrase'].map({1: 'Kids are talking by the door', 2: 'Dogs are sitting by the door'})\n",
    "df['path'] = df['path'].apply(lambda x: x[0] + '/' + x[1])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actors</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       actors  path\n",
       "count     0.0   0.0\n",
       "mean      NaN   NaN\n",
       "std       NaN   NaN\n",
       "min       NaN   NaN\n",
       "25%       NaN   NaN\n",
       "50%       NaN   NaN\n",
       "75%       NaN   NaN\n",
       "max       NaN   NaN"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualizando a distribuição\n",
    "\n",
    "# Visualiza a distribuição das emoções\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17628\\2082103525.py:2: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  df.to_csv(os.path.join(\"D:\\Dados\\Material_complementar_reconhecimento_voz\\Audio_Speech_Actors\", 'Audio_Actors_metadata.csv'), index = False)\n"
     ]
    }
   ],
   "source": [
    "# Cria um arquivo csv com os dados da distribuição\n",
    "df.to_csv(os.path.join(\"D:\\Dados\\Material_complementar_reconhecimento_voz\\Audio_Speech_Actors\", 'Audio_Actors_metadata.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza a quantidade de áudios para cada emoção\n",
    "df.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização gráfica\n",
    "sns.countplot(df, x='emotion');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizando os dados de áudio\n",
    "\n",
    "# Visualiza formas de onda e espectrogramas dos dados de áudio\n",
    "n_files = df.shape[0]\n",
    "rnd = np.random.randint(0, n_files)\n",
    "\n",
    "fname = df.path[rnd]\n",
    "data, sample_rate = librosa.load(fname, sr=None)\n",
    "\n",
    "print('Canais:', data.shape)\n",
    "print('Número total de amostras:', data.shape[0])\n",
    "print('Arquivo: ', fname)\n",
    "print('Taxa de amostragem:', sample_rate)\n",
    "print('Duração: ', len(data) / sample_rate)\n",
    "\n",
    "info = df.iloc[rnd].values\n",
    "title_txt = f'voz: {info[4]} - emoção: {info[0]} ({info[1]}, {info[3]})'\n",
    "plt.title(title_txt.upper(), size=16)\n",
    "ld.waveshow(data, sr=sample_rate)\n",
    "Audio(data = data, rate = sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escolha aleatória de visualizações de formas de onda e espectrogramas de cada emoção\n",
    "random_samples = df.groupby('emotion').sample(1)\n",
    "audio_samples, labels = random_samples['path'].tolist(), random_samples['emotion'].tolist()\n",
    "\n",
    "rows=4\n",
    "cols=2\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(15,15))\n",
    "index = 0\n",
    "for col in range(cols):\n",
    "    for row in range(rows):\n",
    "        data, sample_rate = librosa.load(audio_samples[index], sr = None)\n",
    "        librosa.display.waveshow(data, sr=sample_rate, ax=axs[row][col])\n",
    "        axs[row][col].set_title('{}'.format(labels[index]))\n",
    "        index += 1\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Espectrogramas de STFT\n",
    "\n",
    "# Cria o espectrograma para cada uma das emoções\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(20,20))\n",
    "index = 0\n",
    "for col in range(cols):\n",
    "    for row in range(rows):\n",
    "        data, sample_rate = librosa.load(audio_samples[index], sr = None)\n",
    "        stft = librosa.stft(y = data)\n",
    "        stft_db = librosa.amplitude_to_db(np.abs(stft))\n",
    "        img = librosa.display.specshow(stft_db, x_axis=\"time\", y_axis='log', ax=axs[row][col], cmap = 'Spectral')\n",
    "        axs[row][col].set_title('{}'.format(labels[index]))\n",
    "        fig.colorbar(img, ax=axs[row][col], format='%+2.f dB')\n",
    "        index += 1\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Espectrogramas de MFCCs\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(20,20))\n",
    "index = 0\n",
    "for col in range(cols):\n",
    "    for row in range(rows):\n",
    "        data, sample_rate = librosa.load(audio_samples[index], sr = None)\n",
    "        mfccs = librosa.feature.mfcc(y = data, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_db = librosa.amplitude_to_db(np.abs(mfccs))\n",
    "        img = librosa.display.specshow(mfccs_db, x_axis=\"time\", y_axis='log', ax=axs[row][col], cmap = 'Spectral')\n",
    "        axs[row][col].set_title('{}'.format(labels[index]))\n",
    "        fig.colorbar(img, ax=axs[row][col], format='%+2.f dB')\n",
    "        index += 1\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pré-processamento\n",
    "\n",
    "#Extraindo recursos/características MFCC's de cada arquivo de áudio do dataset\n",
    "# Extrai características MFCC\n",
    "def features_extractor(file_name):\n",
    "    data, sample_rate = librosa.load(file_name, sr = None, res_type = 'kaiser_fast')\n",
    "    mfccs_features = librosa.feature.mfcc(y = data, sr = sample_rate, n_mfcc = 40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T, axis = 0)\n",
    "    return mfccs_scaled_features\n",
    "\n",
    "extracted_features=[]\n",
    "for path in tqdm(df.path.values):\n",
    "  data = features_extractor(path)\n",
    "  extracted_features.append([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertendo os recursos extraídos para visualização com Pandas\n",
    "\n",
    "# Converte os recursos extraídos em DataFrame para visualização\n",
    "extracted_features_df = pd.DataFrame(extracted_features, columns = ['feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividindo entre atributos classe(class) e atributos previsores(features)\n",
    "\n",
    "# Divide os dados em atributos de classe e atributos previsores\n",
    "X = np.array(extracted_features_df['feature'].tolist())\n",
    "y = np.array(df.emotion.tolist())\n",
    "\n",
    "# Cria um objeto para converter rótulos categóricos em valores numéricos\n",
    "labelencoder = LabelEncoder()\n",
    "y = to_categorical(labelencoder.fit_transform(y))\n",
    "\n",
    "# Classes identificadas\n",
    "labelencoder.classes_\n",
    "\n",
    "# Divide os dados em conjuntos de treinamento e teste.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Adiciona uma nova dimensão aos dados de treinamento e teste.\n",
    "X_train = X_train[:,:,np.newaxis]\n",
    "X_test = X_test[:,:,np.newaxis]\n",
    "\n",
    "# Quantidade de classes de emoções\n",
    "num_labels = y.shape[1]\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o modelo Criação da estrutura da rede neural\n",
    "\n",
    "Os espectrogramas extraídos dos arquivos de áudio são como imagens 2D, então podemos usar técnicas de classificação de imagens neles, especificamente Redes Neurais Convolucionais (CNN)!\n",
    "\n",
    "A arquitetura desta rede neural foi definida com base em alguns testes realizados para obter o resultado esperado. A estrutura pode ser ajustada livremente e comparada aos resultados desta estrutura.\n",
    "\n",
    "Parâmetros: Sequential, é a classe para criar a rede neural, pois uma rede neural nada mais é que uma sequência de camadas (camada e entrada, camadas ocultas, camada de saída); kernel_size, o tamanho do kernel (matriz) de convolução; activation, função de ativação; input_shape, na primeira camada este é o tamanho dos dados de entrada Camada MaxPooling1D, que vai fazer a extração das características principais; Camada Conv1d, uma rede neural convolucional que realiza a convolução ao longo de apenas uma dimensão; Camada Flatten, para transformar de matriz em vetor; Camada Dense, quando um neurônio de uma camada está ligado a todas os outros neurônios das outras camadas; Dropout, técnica de regularização para diminuir o overfitting; padding='same', indica que adicionamos uma nova coluna composta por somente 0 (zeros) e utilizamos toda a imagem;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o modelo de rede neural\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=(5), activation='relu',input_shape=(X_train.shape[1],1)))\n",
    "\n",
    "model.add(Conv1D(128, kernel_size=(5),activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=(5)))\n",
    "\n",
    "model.add(Conv1D(256, kernel_size=(5),activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=(5)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando o modelo num_epochs, número de épocas de treinamento num_batch_size, isto indica que vamos enviar de 32 em 32 recursos de áudio (32, 64, 96, 128,...8732) ModelCheckpoint, para salvar o modelo enquanto faz o treinamento\n",
    "\n",
    "filepath, caminho onde será salvo o modelo. Para isto temos uma pasta no Drive chamada saved_models verbose, mostrar mensagens enquanto a rede neural é treinada save_best_only = True, para salvar o modelo somente quando houver uma melhora no resultado Variáveis para efetuar a contagem do tempo de treinamento:\n",
    "\n",
    "start, pegando o horário atual de início do treinamento; duration, ao final do treinamento, subtrair a hora atual com hora de início do treinamento. model_history para armazenar o histórico de treinamento:\n",
    "\n",
    "model.fit para fazer o ajuste do pesos ao longo do treinamento X_train, Y_train, dados de treinamento batch_size = num_batch_size que definimos acima epochs = num_epochs que também definimos acima validation_data=(X_test, Y_test), dados de teste para monitorarmos como está o percentual de acerto da rede neural a cada época callbacks=[checkpointer], checkpointer definido anteriormente verbose = 1, para mostrar as mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montagem do Google Drive para acessar arquivos armazenados\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "\n",
    "# Realize o dowload da pasta Material_complementar_reconhecimento_voz.zip do Google Sala de Aula e transfira-a para o seu Google Drive\n",
    "# Localize o caminho da pasta no menu Arquivos, no menu lateral esquerdo\n",
    "import zipfile\n",
    "#path = \"/content/gdrive/MyDrive/Material_complementar_reconhecimento_voz.zip\"\n",
    "#zip_object = zipfile.ZipFile(file=path, mode=\"r\")\n",
    "#zip_object.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treina o modelo conforme o modelo já treinado disponível na pasta modelos do Material_complementar_reconhecimento_voz.zip\n",
    "num_epochs = 50\n",
    "num_batch_size = 64\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='\"D:\\Dados\\Material_complementar_reconhecimento_voz\\Audio_Speech_Actors\"/speech_emotion_recognition.hdf5',\n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "model_history = model.fit(X_train, Y_train, batch_size=num_batch_size, epochs=num_epochs,\n",
    "                          validation_data=(X_test, Y_test), callbacks=[checkpointer], verbose=1)\n",
    "duration = datetime.now() - start\n",
    "print(\"[INFO] treinamento concluído em: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avaliando o modelo\n",
    "\n",
    "# Avalia o desempenho do modelo treinado usando o conjunto de teste.\n",
    "model.evaluate(X_test,Y_test, verbose=0)\n",
    "\n",
    "# Imprime as chaves do dicionário 'history' que contém os registros do histórico de treinamento do modelo.\n",
    "print(model_history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plota o gráfico da precisão (accuracy) e perda (loss) do modelo durante o treinamento e a validação.\n",
    "plt.plot(model_history.history['accuracy'])\n",
    "plt.plot(model_history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparando as classes reais com as classes previstas\n",
    "\n",
    "# Compara classes reais com as previstas\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Converte as probabilidades das previsões em rótulos de classe. Cada previsão contém um conjunto de probabilidades para cada classe.\n",
    "predictions = predictions.argmax(axis=1)\n",
    "\n",
    "# Converte os índices de previsão em inteiros e os achata em um array unidimensional. Isso é necessário para processar as previsões.\n",
    "predictions = predictions.astype(int).flatten()\n",
    "predictions = (labelencoder.inverse_transform((predictions)))\n",
    "predictions = pd.DataFrame({'Classes Previstas': predictions})\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte a matriz de rótulos de teste (Y_test) para obter o índice da classe com a maior probabilidade (classe real) para cada amostra.\n",
    "actual = Y_test.argmax(axis=1)\n",
    "actual = actual.astype(int).flatten()\n",
    "actual = (labelencoder.inverse_transform((actual)))\n",
    "actual = pd.DataFrame({'Classes Reais': actual})\n",
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combina os DataFrames 'actual' e 'predictions' para criar um novo DataFrame chamado 'finaldf'.\n",
    "finaldf = actual.join(predictions)\n",
    "finaldf[140:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exibindo a Matriz de confusão\n",
    "\n",
    "cm = confusion_matrix(actual, predictions)\n",
    "plt.figure(figsize = (12, 10))\n",
    "cm = pd.DataFrame(cm , index = [i for i in labelencoder.classes_] , columns = [i for i in labelencoder.classes_])\n",
    "ax = sns.heatmap(cm, linecolor='white', cmap='Dark2_r', linewidth=1, annot=True, fmt='g')\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.title('Matriz de Confusão', size=20)\n",
    "plt.xlabel('Classes Previstas', size=14)\n",
    "plt.ylabel('Classes Reais', size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizando a taxa e acerto para cada uma das classes\n",
    "\n",
    "print(classification_report(actual, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testando o modelo em arquivos de áudio\n",
    "\n",
    "def getInfo(data, sample_rate):\n",
    "  print('Canais: ', data.shape)\n",
    "  print('Número total de amostras: ', data.shape[0])\n",
    "  print('Taxa de amostragem: ', sample_rate)\n",
    "  print('Duração: ',  len(data) / sample_rate)\n",
    "\n",
    "def predictSound(AUDIO, info = False, plot_waveform = False, plot_spectrogram = False):\n",
    "  audio, sample_rate = librosa.load(AUDIO, sr = None, res_type='kaiser_fast')\n",
    "  mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "  mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "  mfccs_scaled_features = mfccs_scaled_features.reshape(1,-1)\n",
    "  mfccs_scaled_features = mfccs_scaled_features[:,:,np.newaxis]\n",
    "  predictions = model.predict(mfccs_scaled_features)\n",
    "  plt.barh(labelencoder.classes_, predictions[0], color=list('rgbkymc'))\n",
    "  predictions = predictions.argmax(axis=1)\n",
    "  predictions = predictions.astype(int).flatten()\n",
    "  predictions = (labelencoder.inverse_transform((predictions)))\n",
    "  print('Resultado:', predictions)\n",
    "\n",
    "  if info:\n",
    "    getInfo(audio, sample_rate)\n",
    "\n",
    "  if plot_waveform:\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.title('Emoção: ' + str(predictions[0]).upper(), size=16)\n",
    "    plt.xlabel(\"Tempo (segundos) ==>\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    ld.waveshow(audio, sr=sample_rate)\n",
    "\n",
    "  if plot_spectrogram:\n",
    "    plt.figure(figsize=(14,5))\n",
    "    mfccs_db = librosa.amplitude_to_db(np.abs(mfccs))\n",
    "    plt.title('Emoção: ' + str(predictions[0]).upper(), size=16)\n",
    "    ld.specshow(mfccs_db, x_axis=\"time\", y_axis='log', cmap = 'Spectral')\n",
    "    plt.colorbar(format='%+2.f dB')\n",
    "\n",
    "\n",
    "AUDIO = \"/content/Audio_Speech_Actors/Actor_02/03-01-01-01-01-01-02.wav\" # Teste diferentes áudios\n",
    "audio, sample_rate = librosa.load(AUDIO, sr = None, res_type='kaiser_fast')\n",
    "Audio(data = audio, rate = sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictSound(AUDIO, info = True, plot_waveform = True, plot_spectrogram = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
