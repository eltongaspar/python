{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOShOTsD3Pmvif6KBkHUshb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eltongaspar/python/blob/Advpl/9_4_Consolidar_Colab_Voz_e_Audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reconhecimento de comandos de voz\n",
        "\n",
        "image-2.png\n",
        "\n",
        "Base de dados: Mini Speech Commands\n",
        "\n",
        "O conjunto de dados original consiste em mais de 105.000 arquivos de áudio no formato de arquivo de áudio WAV (Waveform) de pessoas dizendo 35 palavras diferentes. Mais detalhes sobre a base de dados podem ser vistos nestes link: speech_commands\n",
        "\n",
        "Para economizar tempo com o carregamento de dados, será usada uma versão menor do conjunto de dados de Comandos de Fala chamada mini_speech_commands que contém clipes de áudio curtos (um segundo ou menos) de 8 comandos: \"down\", \"go\", \"left\", \"no\", \"right\", \"stop\", \"up\" e \"yes\".\n",
        "\n",
        "A taxa de amostragem para este conjunto de dados é de 16kHz."
      ],
      "metadata": {
        "id": "qw-pRMhA7Lsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importando as bibliotecas\n",
        "\n",
        "# Instala a versão específica 0.8.1 da biblioteca librosa.\n",
        "!pip install librosa#==0.8.1\n",
        "!pip install --upgrade numpy\n",
        "\n",
        "# Verifica a versão do librosa\n",
        "#librosa.__version__\n",
        "#print(librosa.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ig0HYlo7QOp",
        "outputId": "05bca730-4b23-4049-da89-b9318737c805"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: 'librosa#==0.8.1'\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importação de bibliotecas e módulos necessários.\n",
        "import glob\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display as ld\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "6Svy7bWJAjuZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Análise exploratória de dados (EDA)\n",
        "\n",
        "#Baixando e carregando o dataset Mini Speech Commands\n",
        "# Baixa e descompacta o dataset Mini Speech Commands.\n",
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip -O mini_speech_commands.zip\n",
        "\n",
        "!unzip mini_speech_commands.zip -d '/content/'\n",
        "!rm mini_speech_commands.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EODjdZd_FGIc",
        "outputId": "a8160723-848d-4761-ca04-72ac2af1c8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-27 01:17:48--  http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.167.207, 172.253.62.207, 172.253.115.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.167.207|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 182082353 (174M) [application/zip]\n",
            "Saving to: ‘mini_speech_commands.zip’\n",
            "\n",
            "mini_speech_command 100%[===================>] 173.65M   126MB/s    in 1.4s    \n",
            "\n",
            "2024-04-27 01:17:49 (126 MB/s) - ‘mini_speech_commands.zip’ saved [182082353/182082353]\n",
            "\n",
            "Archive:  mini_speech_commands.zip\n",
            "replace /content/__MACOSX/._mini_speech_commands? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria uma lista dos comandos disponíveis no dataset.\n",
        "commands = []\n",
        "for name in glob.glob(str('/content/mini_speech_commands') + '/*' + os.path.sep):\n",
        "  print(name.split('/')[-2])\n",
        "  commands.append(name.split('/')[-2])"
      ],
      "metadata": {
        "id": "ywHHTCZWI9dZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria um dicionário para mapear os comandos para índices numéricos.\n",
        "commands_dict = {i: commands.index(i) for i in commands}\n",
        "print(commands_dict)"
      ],
      "metadata": {
        "id": "xF48jDJnMJR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Criando o dataset\n",
        "\n",
        "# Cria uma lista com os caminhos de todos os arquivos de áudio.\n",
        "speech_data_list = []\n",
        "for name in tqdm(glob.glob(str('/content/mini_speech_commands') + '/*/*')):\n",
        "  speech_data_list.append(name)"
      ],
      "metadata": {
        "id": "fqSLhPugMWbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embaralha a lista de dados de fala.\n",
        "random.seed(42)\n",
        "random.shuffle(speech_data_list)"
      ],
      "metadata": {
        "id": "rVQEOw4UNIV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extrai os rótulos dos dados de fala.\n",
        "speech_data_labels = []\n",
        "for audio in tqdm(speech_data_list):\n",
        "  speech_data_labels.append(os.path.dirname(audio).split('/')[-1])"
      ],
      "metadata": {
        "id": "nkls4y3ONZhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte os rótulos de texto para índices numéricos.\n",
        "speech_label_int = []\n",
        "for audio in tqdm(speech_data_labels):\n",
        "  speech_label_int.append(commands_dict[audio])"
      ],
      "metadata": {
        "id": "TyNZeDbaNfRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os dados de áudio com a taxa de amostragem de 16000 Hz.\n",
        "loaded_speech_data = []\n",
        "for audio in tqdm(speech_data_list):\n",
        "  loaded_speech_data.append(librosa.load(audio, sr = 16000))"
      ],
      "metadata": {
        "id": "oTvP5w8JOwK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria um DataFrame com rótulos, dados de áudio carregados e caminhos.\n",
        "df = pd.DataFrame([speech_data_labels, loaded_speech_data, speech_data_list]).T\n",
        "df"
      ],
      "metadata": {
        "id": "YLFwl9TnQdLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = ['command', 'waves', 'path']\n",
        "df"
      ],
      "metadata": {
        "id": "7cuARZj-QpJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantidade de comandos\n",
        "df['command'].value_counts()"
      ],
      "metadata": {
        "id": "U7DvHOISQwp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualiza a distribuição de comandos no dataset.\n",
        "import matplotlib.pyplot as plt\n",
        "command_counts = df['command'].value_counts()\n",
        "\n",
        "plt.bar(command_counts.index, command_counts.values)\n",
        "plt.xlabel('Comando')\n",
        "plt.ylabel('Contagem')\n",
        "plt.title('Contagem por Comando')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4sDqvF8rQ3SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula e visualiza a distribuição da duração das gravações.\n",
        "duration_of_recordings = []\n",
        "for label in commands:\n",
        "  waves = [f for f in os.listdir('/content/mini_speech_commands/' + label) if f.endswith('.wav')]\n",
        "  for wav in waves:\n",
        "    data, sample_rate = librosa.load('/content/mini_speech_commands/' + label + '/' + wav, sr = 16000)\n",
        "    duration_of_recordings.append(float(len(data) / sample_rate))"
      ],
      "metadata": {
        "id": "z28z8DQHRCns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Duração das gravações\n",
        "sns.displot(duration_of_recordings);"
      ],
      "metadata": {
        "id": "wTgylSK0Wv3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visalizando os dados de áudio\n",
        "#Waveforms\n",
        "\n",
        "# Determina o número total de arquivos no DataFrame 'df'.\n",
        "n_files = df.shape[0]\n",
        "rnd = np.random.randint(0, n_files)\n",
        "fname = df.path[rnd]\n",
        "data, sample_rate = librosa.load(fname, sr=16000)\n",
        "\n",
        "print('Canais: ',  len(data.shape))\n",
        "print('Número total de amostras:', data.shape[0])\n",
        "print('Arquivo:', fname)\n",
        "print('Taxa de amostragem:', sample_rate)\n",
        "print('Duração: ', len(data) / sample_rate)\n",
        "\n",
        "# Extrai informações adicionais do DataFrame relacionadas ao arquivo de áudio e exibe a forma de onda (waveform) do áudio.\n",
        "info = df.iloc[rnd].values\n",
        "title_txt = f'Comando: {info[0]}'\n",
        "plt.title(title_txt.upper(), size=16)\n",
        "librosa.display.waveshow(data, sr=sample_rate)\n",
        "Audio(data = data, rate = sample_rate)"
      ],
      "metadata": {
        "id": "StX1KFPPXrAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleciona um exemplo aleatório de cada comando (categoria) do DataFrame 'df'.\n",
        "random_samples = df.groupby('command').sample(1)\n",
        "audio_samples, labels = random_samples['path'].tolist(), random_samples['command'].tolist()\n",
        "\n",
        "# Configura o layout para exibir as formas de onda (waveforms) dos arquivos de áudio selecionados.\n",
        "rows=4\n",
        "cols=2\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(15,15))\n",
        "index = 0\n",
        "# Itera por cada célula do layout, carrega o arquivo de áudio, exibe sua forma de onda e define o título da célula como o rótulo do comando correspondente.\n",
        "for col in range(cols):\n",
        "    for row in range(rows):\n",
        "        data, sample_rate = librosa.load(audio_samples[index], sr = None)\n",
        "        librosa.display.waveshow(data, sr=sample_rate, ax=axs[row][col])\n",
        "        axs[row][col].set_title('{}'.format(labels[index]))\n",
        "        index += 1\n",
        "\n",
        "# Ajusta o layout para garantir que tudo seja exibido de forma adequada.\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "eXeJpIIucuvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Espectrogramas de STFT (transformada de Fourier)\n",
        "# Cria uma nova figura e um array de subplots com 4 linhas e 2 colunas, com tamanho total de 20x20 polegadas.\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(20,20))\n",
        "index = 0\n",
        "# Itera por cada célula do layout para processar e exibir os espectrogramas de STFT (Short-Time Fourier Transform) dos arquivos de áudio selecionados.\n",
        "for col in range(cols):\n",
        "    for row in range(rows):\n",
        "        data, sample_rate = librosa.load(audio_samples[index], sr = None)\n",
        "        stft = librosa.stft(y = data)\n",
        "        stft_db = librosa.amplitude_to_db(np.abs(stft))\n",
        "        img = librosa.display.specshow(stft_db, x_axis=\"time\", y_axis='log', ax=axs[row][col], cmap = 'Spectral')\n",
        "        axs[row][col].set_title('{}'.format(labels[index]))\n",
        "        fig.colorbar(img, ax=axs[row][col], format='%+2.f dB')\n",
        "        index += 1\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "F-8dqPUcdRfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Espectrogramas de MFCCs\n",
        "\n",
        "# Cria uma nova figura e um array de subplots com 4 linhas e 2 colunas, com tamanho total de 20x20 polegadas.\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(20,20))\n",
        "index = 0\n",
        "# Itera por cada célula do layout para processar e exibir os espectrogramas de MFCC (Mel-Frequency Cepstral Coefficients) dos arquivos de áudio selecionados.\n",
        "for col in range(cols):\n",
        "    for row in range(rows):\n",
        "        data, sample_rate = librosa.load(audio_samples[index], sr = None)\n",
        "        mfccs = librosa.feature.mfcc(y = data, sr=sample_rate, n_mfcc=40)\n",
        "        mfccs_db = librosa.amplitude_to_db(np.abs(mfccs))\n",
        "        img = librosa.display.specshow(mfccs_db, x_axis=\"time\", y_axis='log', ax=axs[row][col], cmap = 'Spectral')\n",
        "        axs[row][col].set_title('{}'.format(labels[index]))\n",
        "        fig.colorbar(img, ax=axs[row][col], format='%+2.f dB')\n",
        "        index += 1\n",
        "\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "KMamSJqreAPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pré-processamento\n",
        "#Extraindo recursos/características MFCC's de cada arquivo de áudio do dataset\n",
        "\n",
        "# Inicializa uma lista vazia para armazenar os coeficientes MFCC (Mel-Frequency Cepstral Coefficients) de cada arquivo de áudio carregado.\n",
        "#speech_data_mfcc = []\n",
        "# Itera sobre a lista 'loaded_speech_data', que contém os dados de áudio e as taxas de amostragem correspondentes.\n",
        "#for loaded_audio in tqdm(loaded_speech_data):\n",
        " # speech_data_mfcc.append(librosa.feature.mfcc(loaded_audio[0], loaded_audio[1]))\n",
        "\n",
        "\n",
        "#Ajustes\n",
        " # Inicialize uma lista vazia para armazenar os coeficientes MFCC.\n",
        "#speech_data_mfcc = []\n",
        "\n",
        "# Itere sobre a lista Load_speech_data.\n",
        "#for audio, sample_rate in loaded_speech_data:\n",
        "    # Extraia MFCCs da amostra de áudio atual.\n",
        "    #mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate)\n",
        "    # Anexe os MFCCs extraídos à lista.\n",
        "    #speech_data_mfcc.append(mfccs)"
      ],
      "metadata": {
        "id": "xb8OREGD45sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa uma lista vazia para armazenar os coeficientes MFCC (Mel-Frequency Cepstral Coefficients) de cada arquivo de áudio carregado.\n",
        "#speech_data_mfcc = []\n",
        "# Itera sobre a lista 'loaded_speech_data', que contém os dados de áudio e as taxas de amostragem correspondentes.\n",
        "#for loaded_audio in tqdm(loaded_speech_data):\n",
        "#  speech_data_mfcc.append(librosa.feature.mfcc(loaded_audio[0], loaded_audio[1]))"
      ],
      "metadata": {
        "id": "VQPRXJjgDhEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicialize uma lista vazia para armazenar os coeficientes MFCC.\n",
        "speech_data_mfcc = []\n",
        "\n",
        "# Itere sobre a lista Load_speech_data.\n",
        "for audio, sample_rate in loaded_speech_data:\n",
        "    # Extraia MFCCs da amostra de áudio atual.\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate)\n",
        "    # Anexe os MFCCs extraídos à lista.\n",
        "    speech_data_mfcc.append(mfccs)"
      ],
      "metadata": {
        "id": "CKaAIHgt8SuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Definindo a proporção da base de dados em treinamento, validação e teste\n",
        "\n",
        "#70% (0.7) para treinar;\n",
        "#Para a validação usamos 15% (0.15);\n",
        "#E para teste o restante da base de dados 15% (0.15).\n",
        "\n",
        "speech_data_as_tensor = []\n",
        "# Itera sobre o índice de cada conjunto de coeficientes MFCC em 'speech_data_mfcc'.\n",
        "for index in range(len(speech_data_mfcc)):\n",
        "  mfcc_array = np.copy(speech_data_mfcc[index])\n",
        "  mfcc_array.resize((20,32), refcheck = False)\n",
        "  speech_data_as_tensor.append(tf.expand_dims(tf.convert_to_tensor(mfcc_array), -1))"
      ],
      "metadata": {
        "id": "DCZSc_jLDv6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide os dados de áudio (tensores) em três conjuntos: treinamento, validação e teste.\n",
        "# A divisão é feita com base em índices específicos para garantir proporções apropriadas para cada conjunto.\n",
        "# O conjunto de treinamento consiste nos primeiros 5600 elementos da lista 'speech_data_as_tensor'.\n",
        "# Isso significa que os dados de 0 até 5599 (inclusive) são utilizados para treinamento.\n",
        "training_slice = speech_data_as_tensor[:5600]\n",
        "validation_slice = speech_data_as_tensor[5600:5600 + 1200]\n",
        "testing_slice = speech_data_as_tensor[5600 + 1200:]"
      ],
      "metadata": {
        "id": "bd_HWaA0EEWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria conjuntos de dados de treinamento, validação e teste usando o TensorFlow.\n",
        "# 'tf.data.Dataset.from_tensor_slices' é uma função que cria um objeto Dataset do TensorFlow.\n",
        "# Cada objeto Dataset é composto por pares de elementos, onde cada par contém um tensor de áudio e um rótulo correspondente.\n",
        "# O conjunto de treinamento ('training_dataset') é criado a partir de 'training_slice' e os primeiros 5600 rótulos de 'speech_label_int'.\n",
        "# Isso significa que cada elemento do 'training_dataset' contém um tensor de áudio do conjunto de treinamento e seu rótulo correspondente.\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices((training_slice, speech_label_int[:5600]))\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_slice, speech_label_int[5600:5600+1200]))\n",
        "testing_dataset = tf.data.Dataset.from_tensor_slices((testing_slice, speech_label_int[-1200:]))"
      ],
      "metadata": {
        "id": "ACCAmMUXELPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo o tamanho do lote (batch) para o conjunto de dados de treinamento e validação.\n",
        "batch_size = 10 # Cada lote contém 10 pares de áudio e rótulo.\n",
        "training_dataset = training_dataset.batch(batch_size)\n",
        "validation_dataset = validation_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "__vYNRfPEPTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Criando o modelo\n",
        "num_labels = 10\n",
        "\n",
        "norm_layer = layers.Normalization()\n",
        "model = models.Sequential([\n",
        "                           layers.Input(shape=(20,32,1)),\n",
        "                           layers.Resizing(32,32),\n",
        "                           norm_layer,\n",
        "                           layers.Conv2D(32, 3, activation = 'relu'),\n",
        "                           layers.Conv2D(64, 3, activation='relu'),\n",
        "                           layers.MaxPooling2D(),\n",
        "                           layers.Dropout(0.25),\n",
        "                           layers.Flatten(),\n",
        "                           layers.Dense(128, activation = 'relu'),\n",
        "                           layers.Dropout(0.25),\n",
        "                           layers.Dense(num_labels), # logits\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "IlHaFVdhEa2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurando o processo de compilação do modelo.\n",
        "# 'model.compile()' é usado para configurar o processo de aprendizado antes do treinamento.\n",
        "# Este método configura o otimizador, a função de perda e as métricas para monitorar.\n",
        "model.compile(optimizer = 'adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "m6DD2S_gFg6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Treinando o modelo\n",
        "\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "checkpointer = (tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        "                tf.keras.callbacks.ModelCheckpoint(filepath='/content/saved_models/voice_command_recognition.hdf5',\n",
        "                                                   save_best_only=True))\n",
        "\n",
        "start = datetime.now()\n",
        "model_history = model.fit(training_dataset, validation_data=validation_dataset, batch_size=BATCH_SIZE,\n",
        "                          epochs=EPOCHS, callbacks=[checkpointer],\n",
        ")\n",
        "duration = datetime.now() - start\n",
        "print(\"Treinamento concluído em: \", duration)"
      ],
      "metadata": {
        "id": "6ijn25_YFpP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Avaliando o modelo\n",
        "\n",
        "# Preparando os dados de áudio e rótulos de teste para avaliação do modelo.\n",
        "test_audio_data = []\n",
        "test_label_data = []\n",
        "# Itera sobre cada par de áudio e rótulo no conjunto de dados de teste.\n",
        "for audio, label in testing_dataset:\n",
        "  test_audio_data.append(audio.numpy())\n",
        "  test_label_data.append(label.numpy())"
      ],
      "metadata": {
        "id": "5dSgaX66FpjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertendo as listas de dados de áudio e rótulos de teste para arrays NumPy.\n",
        "test_audio_data = np.array(test_audio_data)\n",
        "test_label_data = np.array(test_label_data)"
      ],
      "metadata": {
        "id": "TPI98FLfFpsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estabelecendo as predições do modelo\n",
        "y_pred = np.argmax(model.predict(test_audio_data), axis = 1)\n",
        "y_true = test_label_data"
      ],
      "metadata": {
        "id": "lhJWKhl9HEHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Taxa de acerto: ', sum(y_pred == y_true) / len(y_true))"
      ],
      "metadata": {
        "id": "W-s0G1zQHEMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exbindo as métricas de precisão e perda do modelo\n",
        "\n",
        "metrics = model_history.history\n",
        "plt.plot(model_history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(model_history.epoch, metrics['accuracy'], metrics['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PsrveJpSHESu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exibindo a Matriz de confusão\n",
        "\n",
        "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx, xticklabels=commands_dict, yticklabels=commands_dict, annot=True, fmt='g', cmap='Set1_r')\n",
        "plt.xlabel('Classes Previstas')\n",
        "plt.ylabel('Calsses Reais')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wmsiEtPlH2BL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}