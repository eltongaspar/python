{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eltongaspar/python/blob/Advpl/Exercicio_77_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj5yTmdsvWV0"
      },
      "source": [
        "Atividade de experimentação 77\n",
        "Reconhecimento de emoções de fala RAVDESS - SER: Speech Emotion Recognition\n",
        "\n",
        "Base de dados: Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)\n",
        "\n",
        "Contém 7356 arquivos (tamanho total: 24,8 GB);\n",
        "O banco de dados contém 24 atores profissionais (12 mulheres, 12 homens), vocalizando duas declarações lexicalmente combinadas em um sotaque norte-americano neutro;\n",
        "falando as expressões em entonações calmas, felizes, tristes, zangadas, com medo, surpresa e desgosto\n",
        "ou cantando as expressões em entonações calmas, felizes, tristes, raivosas e temerosas\n",
        "assim totalizando 8 emoções;\n",
        "image.png\n",
        "\n",
        "Cada expressão é produzida em dois níveis de intensidade emocional (normal, forte), com uma expressão neutra adicional.\n",
        "\n",
        "Toda a base de dados está subdividida em 3 modalidades:\n",
        "\n",
        "somente áudio (16 bits, 48kHz .wav)\n",
        "áudio-vídeo (720p H.264, AAC 48kHz, .mp4)\n",
        "somente vídeo (sem som).\n",
        "Nota: Não há arquivos de música para Actor_18.\n",
        "\n",
        "Mais detalhes sobre a base de dados podem ser vistos na página oficial RAVDESS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5avrtgkyvYcy",
        "outputId": "37b9d9e7-fa5c-4485-f4cd-86cd69a7cb6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] tensorflow: 2.16.1\n"
          ]
        }
      ],
      "source": [
        "#Importando as bibliotecas\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "from datetime import datetime\n",
        "import librosa\n",
        "import librosa.display as ld\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import Audio\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "print(f'[INFO] tensorflow: {tf.__version__}')\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import (Activation, Conv1D, Dense, Dropout, Flatten, MaxPooling1D)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leWFI2VlwRP9"
      },
      "source": [
        "Análise exploratória de dados (EDA)\n",
        "Baixando e carregando os datasets: SPEECH/SONG\n",
        "\n",
        "Neste projeto serão usados somente os arquivos de áudio contendo as expressões faladas(speech) e cantadas(song) devido a limitação de espaço disponível no Google Drive.\n",
        "\n",
        "arquivos de fala (Audio_Speech_Actors) contém 1440 arquivos;\n",
        "arquivos de música (Audio_Song_Actors) contém 1012 arquivos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziKHD8KAwWJF",
        "outputId": "75832555-e498-4205-f17b-6f9b526f62c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' n�o � reconhecido como um comando interno\n",
            "ou externo, um programa oper�vel ou um arquivo em lotes.\n",
            "'unzip' n�o � reconhecido como um comando interno\n",
            "ou externo, um programa oper�vel ou um arquivo em lotes.\n",
            "'rm' n�o � reconhecido como um comando interno\n",
            "ou externo, um programa oper�vel ou um arquivo em lotes.\n"
          ]
        }
      ],
      "source": [
        "# Código para baixar e extrair o dataset de áudio\n",
        "!wget https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip -O Audio_Speech_Actors_01-24.zip\n",
        "!unzip Audio_Speech_Actors_01-24.zip -d '/content/Audio_Speech_Actors'\n",
        "!rm Audio_Speech_Actors_01-24.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQqx9KVBzTvq"
      },
      "source": [
        "Identificadores de nome de arquivo\n",
        "Cada um dos 7356 arquivos RAVDESS possui um nome de arquivo exclusivo. O nome do arquivo consiste em um identificador numérico de 7 partes (por exemplo, 03-02-01-01-01-01-01.wav). Esses identificadores definem as características do estímulo.\n",
        "\n",
        "Modalidade (01 = AV completo, 02 = apenas vídeo, 03 = apenas áudio).\n",
        "Canal vocal (01 = fala, 02 = música).\n",
        "Emoção (01 = neutro, 02 = calmo, 03 = feliz, 04 = triste, 05 = zangado, 06 = com medo, 07 = nojo, 08 = surpreso).\n",
        "Intensidade emocional (01 = normal, 02 = forte). NOTA: Não há intensidade forte para a emoção 'neutra'.\n",
        "Frase (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
        "Repetição (01 = 1ª repetição, 02 = 2ª repetição).\n",
        "Ator (01 a 24. Os atores com números ímpares são homens, os atores com números pares são mulheres).\n",
        "Exemplo para o arquivo Audio_Song_Actors/Actor_01/03-02-01-01-01-01-01.wav:\n",
        "\n",
        "Modalidade 03: Apenas áudio\n",
        "Canal vocal 02: música\n",
        "Emoção 01: neutro\n",
        "Intensidade emocional 01: normal. NOTA: Não há intensidade forte para a emoção 'neutra'.\n",
        "Frase 01: \"Kids are talking by the door\"\n",
        "Repetição 01: 1ª repetição\n",
        "Ator 01: 1º ator - homem, já que o número de identificação do ator é impar'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NAE3Epa-zPaO"
      },
      "outputs": [],
      "source": [
        "#Criando os datasets\n",
        "modalitys = [] # Modalidade (01 = AV completo, 02 = apenas vídeo, 03 = apenas áudio).\n",
        "voc_channels = [] # Canal vocal (01 = fala, 02 = música).\n",
        "emotions = [] # Emoção (01 = neutro, 02 = calma, 03 = feliz, 04 = triste, 05 = zangado, 06 = com medo, 07 = nojo, 08 = surpreso).\n",
        "intensitys = [] # Intensidade emocional (01 = normal, 02 = forte). NOTA: Não há intensidade forte para a emoção 'neutra'.\n",
        "phrases =[] # Frase (01 = \"Crianças conversam perto da porta\", 02 = \"Cachorros estão sentados na porta\").\n",
        "actors = [] # Ator (01 a 24. Os atores com números ímpares são homens, os atores com números pares são mulheres)\n",
        "\n",
        "full_path = []\n",
        "\n",
        "# Função para criar o dataset\n",
        "def create_dataset(dataset):\n",
        "  for root, dirs, files in tqdm(os.walk(dataset)):\n",
        "    for file in files:\n",
        "      try:\n",
        "        modal = int(file[1:2])\n",
        "        vchan = int(file[4:5])\n",
        "        label = int(file[7:8])\n",
        "        ints = int(file[10:11])\n",
        "        phr = int(file[13:14])\n",
        "        act = int(file[19:20])\n",
        "\n",
        "        modalitys.append(modal)\n",
        "        voc_channels.append(vchan)\n",
        "        emotions.append(label)\n",
        "        intensitys.append(ints)\n",
        "        phrases.append(phr)\n",
        "        actors.append(act)\n",
        "\n",
        "        full_path.append((root, file))\n",
        "      except ValueError:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PuGKASB0zjRg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "# Cria datasets a partir dos arquivos de áudio\n",
        "create_dataset('/content/Audio_Speech_Actors')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T8xdKtAGzl9F"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Mostra o tamanho do dataset de áudio\n",
        "len(full_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zbMnE-bszqPS"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{1: 'neutra',\n",
              " 2: 'calma',\n",
              " 3: 'feliz',\n",
              " 4: 'triste',\n",
              " 5: 'nervosa',\n",
              " 6: 'medo',\n",
              " 7: 'nojo',\n",
              " 8: 'surpreso'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Mapeia emoções para rótulos textuais\n",
        "emotions_list = ['neutra', 'calma', 'feliz', 'triste', 'nervosa', 'medo', 'nojo', 'surpreso']\n",
        "emotion_dict = {em[0] + 1:em[1] for em in enumerate(emotions_list)}\n",
        "emotion_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "l9NFQL-9ztgR"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>voc_channel</th>\n",
              "      <th>modality</th>\n",
              "      <th>intensity</th>\n",
              "      <th>actors</th>\n",
              "      <th>phrase</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [emotion, voc_channel, modality, intensity, actors, phrase, path]\n",
              "Index: []"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cria DataFrame com as informações extraídas\n",
        "df = pd.DataFrame([emotions, voc_channels, modalitys, intensitys, actors, phrases, full_path]).T\n",
        "df.columns = ['emotion', 'voc_channel', 'modality', 'intensity', 'actors', 'phrase', 'path']\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uCNOy-ymzyOl"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>voc_channel</th>\n",
              "      <th>modality</th>\n",
              "      <th>intensity</th>\n",
              "      <th>actors</th>\n",
              "      <th>phrase</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [emotion, voc_channel, modality, intensity, actors, phrase, path]\n",
              "Index: []"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Mapeia valores numéricos para rótulos de texto no DataFrame\n",
        "df['emotion'] = df['emotion'].map(emotion_dict)\n",
        "df['voc_channel'] = df['voc_channel'].map({1: 'fala', 2: 'musica'})\n",
        "df['modality'] = df['modality'].map({1: 'AV completo', 2: 'apenas video', 3: 'apenas audio'})\n",
        "df['intensity'] = df['intensity'].map({1: 'normal', 2: 'forte'})\n",
        "df['actors'] = df['actors'].apply({lambda x: 'feminino' if x % 2 == 0 else 'masculino'})\n",
        "df['phrase'] = df['phrase'].map({1: 'Kids are talking by the door', 2: 'Dogs are sitting by the door'})\n",
        "df['path'] = df['path'].apply(lambda x: x[0] + '/' + x[1])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gSAII41iz7kd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actors</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       actors  path\n",
              "count     0.0   0.0\n",
              "mean      NaN   NaN\n",
              "std       NaN   NaN\n",
              "min       NaN   NaN\n",
              "25%       NaN   NaN\n",
              "50%       NaN   NaN\n",
              "75%       NaN   NaN\n",
              "max       NaN   NaN"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Visualizando a distribuição\n",
        "\n",
        "# Visualiza a distribuição das emoções\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9-ZyzsmR0Brp"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Cannot save file into a non-existent directory: '\\content'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Cria um arquivo csv com os dados da distribuição\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAudio_Actors_metadata.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\pandas\\core\\generic.py:3964\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3953\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3955\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3956\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3957\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3961\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3962\u001b[0m )\n\u001b[1;32m-> 3964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3967\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
            "File \u001b[1;32md:\\Python\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '\\content'"
          ]
        }
      ],
      "source": [
        "# Cria um arquivo csv com os dados da distribuição\n",
        "df.to_csv(os.path.join('/content', 'Audio_Actors_metadata.csv'), index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxpUnbfB0ERW"
      },
      "outputs": [],
      "source": [
        "# Visualiza a quantidade de áudios para cada emoção\n",
        "df.emotion.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrKpIkcW0Gjr"
      },
      "outputs": [],
      "source": [
        "# Visualização gráfica\n",
        "sns.countplot(df, x='emotion');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5N6ZfcN5YIl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xsmCoXQ0ID1"
      },
      "outputs": [],
      "source": [
        "#Visualizando os dados de áudio\n",
        "\n",
        "# Visualiza formas de onda e espectrogramas dos dados de áudio\n",
        "n_files = df.shape[0]\n",
        "rnd = np.random.randint(0, n_files)\n",
        "\n",
        "fname = df.path[rnd]\n",
        "data, sample_rate = librosa.load(fname, sr=None)\n",
        "\n",
        "print('Canais:', data.shape)\n",
        "print('Número total de amostras:', data.shape[0])\n",
        "print('Arquivo: ', fname)\n",
        "print('Taxa de amostragem:', sample_rate)\n",
        "print('Duração: ', len(data) / sample_rate)\n",
        "\n",
        "info = df.iloc[rnd].values\n",
        "title_txt = f'voz: {info[4]} - emoção: {info[0]} ({info[1]}, {info[3]})'\n",
        "plt.title(title_txt.upper(), size=16)\n",
        "ld.waveshow(data, sr=sample_rate)\n",
        "Audio(data = data, rate = sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z04wQCG0VmE"
      },
      "outputs": [],
      "source": [
        "# Escolha aleatória de visualizações de formas de onda e espectrogramas de cada emoção\n",
        "random_samples = df.groupby('emotion').sample(1)\n",
        "audio_samples, labels = random_samples['path'].tolist(), random_samples['emotion'].tolist()\n",
        "\n",
        "rows=4\n",
        "cols=2\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(15,15))\n",
        "index = 0\n",
        "for col in range(cols):\n",
        "    for row in range(rows):\n",
        "        data, sample_rate = librosa.load(audio_samples[index], sr = None)\n",
        "        librosa.display.waveshow(data, sr=sample_rate, ax=axs[row][col])\n",
        "        axs[row][col].set_title('{}'.format(labels[index]))\n",
        "        index += 1\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kkdEAY-0cwm"
      },
      "outputs": [],
      "source": [
        "#Espectrogramas de STFT\n",
        "\n",
        "# Cria o espectrograma para cada uma das emoções\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(20,20))\n",
        "index = 0\n",
        "for col in range(cols):\n",
        "    for row in range(rows):\n",
        "        data, sample_rate = librosa.load(audio_samples[index], sr = None)\n",
        "        stft = librosa.stft(y = data)\n",
        "        stft_db = librosa.amplitude_to_db(np.abs(stft))\n",
        "        img = librosa.display.specshow(stft_db, x_axis=\"time\", y_axis='log', ax=axs[row][col], cmap = 'Spectral')\n",
        "        axs[row][col].set_title('{}'.format(labels[index]))\n",
        "        fig.colorbar(img, ax=axs[row][col], format='%+2.f dB')\n",
        "        index += 1\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-HJOmR90oma"
      },
      "outputs": [],
      "source": [
        "#Espectrogramas de MFCCs\n",
        "\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(20,20))\n",
        "index = 0\n",
        "for col in range(cols):\n",
        "    for row in range(rows):\n",
        "        data, sample_rate = librosa.load(audio_samples[index], sr = None)\n",
        "        mfccs = librosa.feature.mfcc(y = data, sr=sample_rate, n_mfcc=40)\n",
        "        mfccs_db = librosa.amplitude_to_db(np.abs(mfccs))\n",
        "        img = librosa.display.specshow(mfccs_db, x_axis=\"time\", y_axis='log', ax=axs[row][col], cmap = 'Spectral')\n",
        "        axs[row][col].set_title('{}'.format(labels[index]))\n",
        "        fig.colorbar(img, ax=axs[row][col], format='%+2.f dB')\n",
        "        index += 1\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikhGKKz006Z8"
      },
      "outputs": [],
      "source": [
        "#Pré-processamento\n",
        "\n",
        "#Extraindo recursos/características MFCC's de cada arquivo de áudio do dataset\n",
        "# Extrai características MFCC\n",
        "def features_extractor(file_name):\n",
        "    data, sample_rate = librosa.load(file_name, sr = None, res_type = 'kaiser_fast')\n",
        "    mfccs_features = librosa.feature.mfcc(y = data, sr = sample_rate, n_mfcc = 40)\n",
        "    mfccs_scaled_features = np.mean(mfccs_features.T, axis = 0)\n",
        "    return mfccs_scaled_features\n",
        "\n",
        "extracted_features=[]\n",
        "for path in tqdm(df.path.values):\n",
        "  data = features_extractor(path)\n",
        "  extracted_features.append([data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JT_pcUWr5bur"
      },
      "outputs": [],
      "source": [
        "#Convertendo os recursos extraídos para visualização com Pandas\n",
        "\n",
        "# Converte os recursos extraídos em DataFrame para visualização\n",
        "extracted_features_df = pd.DataFrame(extracted_features, columns = ['feature'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxuw7Rcn5hqN"
      },
      "outputs": [],
      "source": [
        "#Dividindo entre atributos classe(class) e atributos previsores(features)\n",
        "\n",
        "# Divide os dados em atributos de classe e atributos previsores\n",
        "X = np.array(extracted_features_df['feature'].tolist())\n",
        "y = np.array(df.emotion.tolist())\n",
        "\n",
        "# Cria um objeto para converter rótulos categóricos em valores numéricos\n",
        "labelencoder = LabelEncoder()\n",
        "y = to_categorical(labelencoder.fit_transform(y))\n",
        "\n",
        "# Classes identificadas\n",
        "labelencoder.classes_\n",
        "\n",
        "# Divide os dados em conjuntos de treinamento e teste.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Adiciona uma nova dimensão aos dados de treinamento e teste.\n",
        "X_train = X_train[:,:,np.newaxis]\n",
        "X_test = X_test[:,:,np.newaxis]\n",
        "\n",
        "# Quantidade de classes de emoções\n",
        "num_labels = y.shape[1]\n",
        "num_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0WVhDSh51F0"
      },
      "source": [
        "Criando o modelo\n",
        "Criação da estrutura da rede neural\n",
        "\n",
        "Os espectrogramas extraídos dos arquivos de áudio são como imagens 2D, então podemos usar técnicas de classificação de imagens neles, especificamente Redes Neurais Convolucionais (CNN)!\n",
        "\n",
        "A arquitetura desta rede neural foi definida com base em alguns testes realizados para obter o resultado esperado. A estrutura pode ser ajustada livremente e comparada aos resultados desta estrutura.\n",
        "\n",
        "Parâmetros:\n",
        "Sequential, é a classe para criar a rede neural, pois uma rede neural nada mais é que uma sequência de camadas (camada e entrada, camadas ocultas, camada de saída);\n",
        "kernel_size, o tamanho do kernel (matriz) de convolução;\n",
        "activation, função de ativação;\n",
        "input_shape, na primeira camada este é o tamanho dos dados de entrada\n",
        "Camada MaxPooling1D, que vai fazer a extração das características principais;\n",
        "Camada Conv1d, uma rede neural convolucional que realiza a convolução ao longo de apenas uma dimensão;\n",
        "Camada Flatten, para transformar de matriz em vetor;\n",
        "Camada Dense, quando um neurônio de uma camada está ligado a todas os outros neurônios das outras camadas;\n",
        "Dropout, técnica de regularização para diminuir o overfitting;\n",
        "padding='same', indica que adicionamos uma nova coluna composta por somente 0 (zeros) e utilizamos toda a imagem;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlwG6H_i57Zy"
      },
      "outputs": [],
      "source": [
        "# Cria o modelo de rede neural\n",
        "model=Sequential()\n",
        "\n",
        "model.add(Conv1D(64, kernel_size=(5), activation='relu',input_shape=(X_train.shape[1],1)))\n",
        "\n",
        "model.add(Conv1D(128, kernel_size=(5),activation='relu', padding='same'))\n",
        "model.add(MaxPooling1D(pool_size=(5)))\n",
        "\n",
        "model.add(Conv1D(256, kernel_size=(5),activation='relu', padding='same'))\n",
        "model.add(MaxPooling1D(pool_size=(5)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsmDJf1D6DLN"
      },
      "source": [
        "Treinando o modelo\n",
        "num_epochs, número de épocas de treinamento\n",
        "num_batch_size, isto indica que vamos enviar de 32 em 32 recursos de áudio (32, 64, 96, 128,...8732)\n",
        "ModelCheckpoint, para salvar o modelo enquanto faz o treinamento\n",
        "\n",
        "filepath, caminho onde será salvo o modelo. Para isto temos uma pasta no Drive chamada saved_models\n",
        "verbose, mostrar mensagens enquanto a rede neural é treinada\n",
        "save_best_only = True, para salvar o modelo somente quando houver uma melhora no resultado\n",
        "Variáveis para efetuar a contagem do tempo de treinamento:\n",
        "\n",
        "start, pegando o horário atual de início do treinamento;\n",
        "duration, ao final do treinamento, subtrair a hora atual com hora de início do treinamento.\n",
        "model_history para armazenar o histórico de treinamento:\n",
        "\n",
        "model.fit para fazer o ajuste do pesos ao longo do treinamento\n",
        "X_train, Y_train, dados de treinamento\n",
        "batch_size = num_batch_size que definimos acima\n",
        "epochs = num_epochs que também definimos acima\n",
        "validation_data=(X_test, Y_test), dados de teste para monitorarmos como está o percentual de acerto da rede neural a cada época\n",
        "callbacks=[checkpointer], checkpointer definido anteriormente\n",
        "verbose = 1, para mostrar as mensagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDgxZ6GJ6EKk"
      },
      "outputs": [],
      "source": [
        "# Montagem do Google Drive para acessar arquivos armazenados\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Realize o dowload da pasta Material_complementar_reconhecimento_voz.zip do Google Sala de Aula e transfira-a para o seu Google Drive\n",
        "# Localize o caminho da pasta no menu Arquivos, no menu lateral esquerdo\n",
        "import zipfile\n",
        "#path = \"/content/gdrive/MyDrive/Material_complementar_reconhecimento_voz.zip\"\n",
        "#zip_object = zipfile.ZipFile(file=path, mode=\"r\")\n",
        "#zip_object.extractall(\"./\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAg2NKFG6UMQ"
      },
      "outputs": [],
      "source": [
        "# Treina o modelo conforme o modelo já treinado disponível na pasta modelos do Material_complementar_reconhecimento_voz.zip\n",
        "num_epochs = 50\n",
        "num_batch_size = 64\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='/content/modelos/speech_emotion_recognition.hdf5',\n",
        "                               verbose=1, save_best_only=True)\n",
        "start = datetime.now()\n",
        "model_history = model.fit(X_train, Y_train, batch_size=num_batch_size, epochs=num_epochs,\n",
        "                          validation_data=(X_test, Y_test), callbacks=[checkpointer], verbose=1)\n",
        "duration = datetime.now() - start\n",
        "print(\"[INFO] treinamento concluído em: \", duration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OH-P12BX6ki_"
      },
      "outputs": [],
      "source": [
        "#Avaliando o modelo\n",
        "\n",
        "# Avalia o desempenho do modelo treinado usando o conjunto de teste.\n",
        "model.evaluate(X_test,Y_test, verbose=0)\n",
        "\n",
        "# Imprime as chaves do dicionário 'history' que contém os registros do histórico de treinamento do modelo.\n",
        "print(model_history.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZyzi_Ga6qa6"
      },
      "outputs": [],
      "source": [
        "# Plota o gráfico da precisão (accuracy) e perda (loss) do modelo durante o treinamento e a validação.\n",
        "plt.plot(model_history.history['accuracy'])\n",
        "plt.plot(model_history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(model_history.history['loss'])\n",
        "plt.plot(model_history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_iI6q7r62Oe"
      },
      "outputs": [],
      "source": [
        "#Comparando as classes reais com as classes previstas\n",
        "\n",
        "# Compara classes reais com as previstas\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Converte as probabilidades das previsões em rótulos de classe. Cada previsão contém um conjunto de probabilidades para cada classe.\n",
        "predictions = predictions.argmax(axis=1)\n",
        "\n",
        "# Converte os índices de previsão em inteiros e os achata em um array unidimensional. Isso é necessário para processar as previsões.\n",
        "predictions = predictions.astype(int).flatten()\n",
        "predictions = (labelencoder.inverse_transform((predictions)))\n",
        "predictions = pd.DataFrame({'Classes Previstas': predictions})\n",
        "predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bke0K9Rb7DXu"
      },
      "outputs": [],
      "source": [
        "# Converte a matriz de rótulos de teste (Y_test) para obter o índice da classe com a maior probabilidade (classe real) para cada amostra.\n",
        "actual = Y_test.argmax(axis=1)\n",
        "actual = actual.astype(int).flatten()\n",
        "actual = (labelencoder.inverse_transform((actual)))\n",
        "actual = pd.DataFrame({'Classes Reais': actual})\n",
        "actual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxtF4dfa7JEt"
      },
      "outputs": [],
      "source": [
        "# Combina os DataFrames 'actual' e 'predictions' para criar um novo DataFrame chamado 'finaldf'.\n",
        "finaldf = actual.join(predictions)\n",
        "finaldf[140:150]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i15CJGmY7MV9"
      },
      "outputs": [],
      "source": [
        "#Exibindo a Matriz de confusão\n",
        "\n",
        "cm = confusion_matrix(actual, predictions)\n",
        "plt.figure(figsize = (12, 10))\n",
        "cm = pd.DataFrame(cm , index = [i for i in labelencoder.classes_] , columns = [i for i in labelencoder.classes_])\n",
        "ax = sns.heatmap(cm, linecolor='white', cmap='Dark2_r', linewidth=1, annot=True, fmt='g')\n",
        "bottom, top = ax.get_ylim()\n",
        "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
        "plt.title('Matriz de Confusão', size=20)\n",
        "plt.xlabel('Classes Previstas', size=14)\n",
        "plt.ylabel('Classes Reais', size=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUOteDc37R_g"
      },
      "outputs": [],
      "source": [
        "#Visualizando a taxa e acerto para cada uma das classes\n",
        "\n",
        "print(classification_report(actual, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hc885s2x7Xdl"
      },
      "outputs": [],
      "source": [
        "#Testando o modelo em arquivos de áudio\n",
        "\n",
        "def getInfo(data, sample_rate):\n",
        "  print('Canais: ', data.shape)\n",
        "  print('Número total de amostras: ', data.shape[0])\n",
        "  print('Taxa de amostragem: ', sample_rate)\n",
        "  print('Duração: ',  len(data) / sample_rate)\n",
        "\n",
        "def predictSound(AUDIO, info = False, plot_waveform = False, plot_spectrogram = False):\n",
        "  audio, sample_rate = librosa.load(AUDIO, sr = None, res_type='kaiser_fast')\n",
        "  mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "  mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
        "  mfccs_scaled_features = mfccs_scaled_features.reshape(1,-1)\n",
        "  mfccs_scaled_features = mfccs_scaled_features[:,:,np.newaxis]\n",
        "  predictions = model.predict(mfccs_scaled_features)\n",
        "  plt.barh(labelencoder.classes_, predictions[0], color=list('rgbkymc'))\n",
        "  predictions = predictions.argmax(axis=1)\n",
        "  predictions = predictions.astype(int).flatten()\n",
        "  predictions = (labelencoder.inverse_transform((predictions)))\n",
        "  print('Resultado:', predictions)\n",
        "\n",
        "  if info:\n",
        "    getInfo(audio, sample_rate)\n",
        "\n",
        "  if plot_waveform:\n",
        "    plt.figure(figsize=(14,5))\n",
        "    plt.title('Emoção: ' + str(predictions[0]).upper(), size=16)\n",
        "    plt.xlabel(\"Tempo (segundos) ==>\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    ld.waveshow(audio, sr=sample_rate)\n",
        "\n",
        "  if plot_spectrogram:\n",
        "    plt.figure(figsize=(14,5))\n",
        "    mfccs_db = librosa.amplitude_to_db(np.abs(mfccs))\n",
        "    plt.title('Emoção: ' + str(predictions[0]).upper(), size=16)\n",
        "    ld.specshow(mfccs_db, x_axis=\"time\", y_axis='log', cmap = 'Spectral')\n",
        "    plt.colorbar(format='%+2.f dB')\n",
        "\n",
        "\n",
        "AUDIO = \"/content/Audio_Speech_Actors/Actor_02/03-01-01-01-01-01-02.wav\" # Teste diferentes áudios\n",
        "audio, sample_rate = librosa.load(AUDIO, sr = None, res_type='kaiser_fast')\n",
        "Audio(data = audio, rate = sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0n7yXm2U7mJZ"
      },
      "outputs": [],
      "source": [
        "predictSound(AUDIO, info = True, plot_waveform = True, plot_spectrogram = True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNp1r1MFBpxM5ApHS4z9Vva",
      "history_visible": true,
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
